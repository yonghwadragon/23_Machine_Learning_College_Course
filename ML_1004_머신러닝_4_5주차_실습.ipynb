{"cells":[{"cell_type":"markdown","metadata":{"id":"fLR3HxPj1fep"},"source":["# I. 하이퍼 파라미터 최적화: CV"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gb3PfgOw7DUI"},"outputs":[],"source":["#import packages\n","import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_iris\n","\n","#Loading iris dataset from sklearn\n","iris = load_iris()\n","\n","#independent feautres\n","X = iris.data\n","\n","# target features\n","y = iris.target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7SWSd_N7VJI"},"outputs":[],"source":["#import XGboost\n","from xgboost import XGBClassifier\n","\n","#Defining XGB Classification model\n","clf = XGBClassifier()"]},{"cell_type":"markdown","metadata":{"id":"7b4MwEbA7c6o"},"source":["### 1.Grid SearchCV\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8j47oJZ7cYv"},"outputs":[],"source":["#Importing packages from sklearn\n","\n","from sklearn import preprocessing\n","from sklearn import model_selection\n","from sklearn import metrics\n","\n","#defining a set of values as a dictionary for hyperparameters\n","\n","param_grid = {\n","    \"n_estimators\":[100,200,300,400],\n","    \"max_depth\":[1,3,5,7],                                 #4개 4개 16가지\n","    #\"reg_lambda\":[.01,.1,.5] # 무시하자\n","}\n","\n","#declaring GridSearchCV model\n","\n","model = model_selection.GridSearchCV(\n","    estimator = clf,  #분류모형\n","    param_grid = param_grid,  #파라미터 범위\n","    scoring = 'accuracy',  #정분류율\n","    verbose = 10,\n","    n_jobs = 1,          # 계산하는 코어의 수 지정, 병렬 처리\n","    cv = 5         # 5-fold CV\n",") # 4 * 4 * 5 번 계산\n","\n","#fitting values to the gridsearchcv model\n","\n","model.fit(X,y)\n","#printing the best possible values to enhance accuracy\n","print(model.best_params_)\n","print(model.best_estimator_) # best 모형\n","#printing the best score\n","print(model.best_score_)"]},{"cell_type":"markdown","metadata":{"id":"utwaZq0g7o7o"},"source":["### 2. RandomizedSearchCV\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Y6qy6zT7nxh"},"outputs":[],"source":["#defining a set of values as a dictionary for hyperparameters\n","param_grid = {\n","    \"n_estimators\":[100,200,300,400],\n","    \"max_depth\":[1,3,5,7],\n","    #\"reg_lambda\":[.01,.1,.5]\n","}\n","\n","#declaring RandomizedSearchCV model\n","model = model_selection.RandomizedSearchCV(\n","    estimator = clf,\n","    param_distributions = param_grid,\n","    scoring = 'accuracy',\n","    verbose = 10,\n","    n_jobs = 1,\n","    cv = 5,\n","    n_iter=10\n",")\n","\n","#fitting values to the RandomizedSearchCV model\n","model.fit(X,y)\n","\n","#printing the best possible values to enhance accuracy\n","print(model.best_params_)\n","print(model.best_estimator_)\n","#printing the best score\n","print(model.best_score_)"]},{"cell_type":"code","source":["#iris 데이터에 대해서 AdaBoostClassifier()로 분류를 하세요\n","#GridsearchCV를 적용하세요\n","#10 - fold, n-estimators= 80, 90, 100, 110, 120, learning_rate=0.3, 0.6, 1.0\n","#best score, best param출력"],"metadata":{"id":"MffR_Ev4Jc_E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier\n","clf = AdaBoostClassifier()\n","param_grid = {\n","    \"n_estimators\":[80,90,100,110,120], \"learning_rate\":[0.3, 0.6, 1.0]}\n","model = model_selection.GridSearchCV(\n","    estimator = clf,  #분류모형\n","    param_grid = param_grid,  #파라미터 범위\n","    scoring = 'accuracy',  #정분류율\n","    verbose = 10,        # 이걸 0으로하면 결과만 보여준다\n","    n_jobs = 1,          # 계산하는 코어의 수 지정, 병렬 처리\n","    cv = 10\n",")\n","model.fit(X,y)\n","print(model.best_params_)\n","print(model.best_score_)\n","model.best_estimator_  #모형"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"v6bXJHkaKKr8","executionInfo":{"status":"ok","timestamp":1696381100651,"user_tz":-540,"elapsed":28960,"user":{"displayName":"룡룡","userId":"01678869000619300917"}},"outputId":"3c7b106c-2b9b-48a3-8899-012a4eb17c53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 10 folds for each of 15 candidates, totalling 150 fits\n","[CV 1/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 1/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=1.000 total time=   0.2s\n","[CV 2/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 2/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=0.933 total time=   0.2s\n","[CV 3/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 3/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=1.000 total time=   0.2s\n","[CV 4/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 4/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=0.933 total time=   0.1s\n","[CV 5/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 5/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=0.933 total time=   0.1s\n","[CV 6/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 6/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=0.733 total time=   0.1s\n","[CV 7/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 7/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=0.867 total time=   0.1s\n","[CV 8/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 8/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=0.933 total time=   0.1s\n","[CV 9/10; 1/15] START learning_rate=0.3, n_estimators=80........................\n","[CV 9/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 10/10; 1/15] START learning_rate=0.3, n_estimators=80.......................\n","[CV 10/10; 1/15] END learning_rate=0.3, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 1/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 1/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 2/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 2/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 3/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 3/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 4/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 4/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 5/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 5/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 6/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 6/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=0.733 total time=   0.2s\n","[CV 7/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 7/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=0.867 total time=   0.2s\n","[CV 8/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 8/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 9/10; 2/15] START learning_rate=0.3, n_estimators=90........................\n","[CV 9/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 10/10; 2/15] START learning_rate=0.3, n_estimators=90.......................\n","[CV 10/10; 2/15] END learning_rate=0.3, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 1/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 1/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 2/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 2/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 3/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 3/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 4/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 4/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 5/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 5/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 6/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 6/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=0.733 total time=   0.2s\n","[CV 7/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 7/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 8/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 8/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 9/10; 3/15] START learning_rate=0.3, n_estimators=100.......................\n","[CV 9/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 10/10; 3/15] START learning_rate=0.3, n_estimators=100......................\n","[CV 10/10; 3/15] END learning_rate=0.3, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 1/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 1/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 2/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 2/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=0.933 total time=   0.3s\n","[CV 3/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 3/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=1.000 total time=   0.3s\n","[CV 4/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 4/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=0.933 total time=   0.3s\n","[CV 5/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 5/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=0.933 total time=   0.3s\n","[CV 6/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 6/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=0.733 total time=   0.3s\n","[CV 7/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 7/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=0.933 total time=   0.3s\n","[CV 8/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 8/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=0.933 total time=   0.3s\n","[CV 9/10; 4/15] START learning_rate=0.3, n_estimators=110.......................\n","[CV 9/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=1.000 total time=   0.3s\n","[CV 10/10; 4/15] START learning_rate=0.3, n_estimators=110......................\n","[CV 10/10; 4/15] END learning_rate=0.3, n_estimators=110;, score=1.000 total time=   0.3s\n","[CV 1/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 1/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 2/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 2/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 3/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 3/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 4/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 4/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 5/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 5/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 6/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 6/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=0.733 total time=   0.2s\n","[CV 7/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 7/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 8/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 8/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 9/10; 5/15] START learning_rate=0.3, n_estimators=120.......................\n","[CV 9/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 10/10; 5/15] START learning_rate=0.3, n_estimators=120......................\n","[CV 10/10; 5/15] END learning_rate=0.3, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 1/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 1/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 2/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 2/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=0.933 total time=   0.1s\n","[CV 3/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 3/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 4/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 4/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=0.933 total time=   0.1s\n","[CV 5/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 5/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=0.933 total time=   0.1s\n","[CV 6/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 6/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=0.733 total time=   0.1s\n","[CV 7/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 7/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=0.867 total time=   0.1s\n","[CV 8/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 8/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 9/10; 6/15] START learning_rate=0.6, n_estimators=80........................\n","[CV 9/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 10/10; 6/15] START learning_rate=0.6, n_estimators=80.......................\n","[CV 10/10; 6/15] END learning_rate=0.6, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 1/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 1/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 2/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 2/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 3/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 3/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 4/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 4/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 5/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 5/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 6/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 6/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=0.733 total time=   0.2s\n","[CV 7/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 7/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=0.867 total time=   0.2s\n","[CV 8/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 8/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 9/10; 7/15] START learning_rate=0.6, n_estimators=90........................\n","[CV 9/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 10/10; 7/15] START learning_rate=0.6, n_estimators=90.......................\n","[CV 10/10; 7/15] END learning_rate=0.6, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 1/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 1/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 2/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 2/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 3/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 3/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 4/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 4/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 5/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 5/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 6/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 6/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=0.733 total time=   0.2s\n","[CV 7/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 7/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=0.867 total time=   0.2s\n","[CV 8/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 8/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 9/10; 8/15] START learning_rate=0.6, n_estimators=100.......................\n","[CV 9/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 10/10; 8/15] START learning_rate=0.6, n_estimators=100......................\n","[CV 10/10; 8/15] END learning_rate=0.6, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 1/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 1/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 2/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 2/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=0.933 total time=   0.2s\n","[CV 3/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 3/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 4/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 4/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=0.933 total time=   0.2s\n","[CV 5/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 5/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=0.933 total time=   0.2s\n","[CV 6/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 6/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=0.733 total time=   0.2s\n","[CV 7/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 7/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=0.867 total time=   0.2s\n","[CV 8/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 8/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=0.933 total time=   0.2s\n","[CV 9/10; 9/15] START learning_rate=0.6, n_estimators=110.......................\n","[CV 9/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 10/10; 9/15] START learning_rate=0.6, n_estimators=110......................\n","[CV 10/10; 9/15] END learning_rate=0.6, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 1/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 1/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 2/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 2/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 3/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 3/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 4/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 4/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 5/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 5/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 6/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 6/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=0.733 total time=   0.3s\n","[CV 7/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 7/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=0.933 total time=   0.3s\n","[CV 8/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 8/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=1.000 total time=   0.3s\n","[CV 9/10; 10/15] START learning_rate=0.6, n_estimators=120......................\n","[CV 9/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=1.000 total time=   0.3s\n","[CV 10/10; 10/15] START learning_rate=0.6, n_estimators=120.....................\n","[CV 10/10; 10/15] END learning_rate=0.6, n_estimators=120;, score=1.000 total time=   0.3s\n","[CV 1/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 1/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=1.000 total time=   0.2s\n","[CV 2/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 2/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=0.933 total time=   0.2s\n","[CV 3/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 3/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=1.000 total time=   0.2s\n","[CV 4/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 4/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=0.933 total time=   0.2s\n","[CV 5/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 5/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=0.933 total time=   0.2s\n","[CV 6/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 6/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=0.800 total time=   0.2s\n","[CV 7/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 7/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=0.867 total time=   0.1s\n","[CV 8/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 8/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 9/10; 11/15] START learning_rate=1.0, n_estimators=80.......................\n","[CV 9/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 10/10; 11/15] START learning_rate=1.0, n_estimators=80......................\n","[CV 10/10; 11/15] END learning_rate=1.0, n_estimators=80;, score=1.000 total time=   0.1s\n","[CV 1/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 1/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 2/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 2/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 3/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 3/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 4/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 4/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 5/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 5/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=0.933 total time=   0.2s\n","[CV 6/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 6/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=0.867 total time=   0.2s\n","[CV 7/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 7/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=0.867 total time=   0.2s\n","[CV 8/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 8/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 9/10; 12/15] START learning_rate=1.0, n_estimators=90.......................\n","[CV 9/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 10/10; 12/15] START learning_rate=1.0, n_estimators=90......................\n","[CV 10/10; 12/15] END learning_rate=1.0, n_estimators=90;, score=1.000 total time=   0.2s\n","[CV 1/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 1/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 2/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 2/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 3/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 3/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 4/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 4/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 5/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 5/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=0.933 total time=   0.2s\n","[CV 6/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 6/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=0.800 total time=   0.2s\n","[CV 7/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 7/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=0.867 total time=   0.2s\n","[CV 8/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 8/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 9/10; 13/15] START learning_rate=1.0, n_estimators=100......................\n","[CV 9/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 10/10; 13/15] START learning_rate=1.0, n_estimators=100.....................\n","[CV 10/10; 13/15] END learning_rate=1.0, n_estimators=100;, score=1.000 total time=   0.2s\n","[CV 1/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 1/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 2/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 2/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=0.933 total time=   0.2s\n","[CV 3/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 3/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 4/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 4/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=0.933 total time=   0.2s\n","[CV 5/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 5/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=0.933 total time=   0.2s\n","[CV 6/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 6/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=0.867 total time=   0.2s\n","[CV 7/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 7/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=0.867 total time=   0.2s\n","[CV 8/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 8/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 9/10; 14/15] START learning_rate=1.0, n_estimators=110......................\n","[CV 9/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 10/10; 14/15] START learning_rate=1.0, n_estimators=110.....................\n","[CV 10/10; 14/15] END learning_rate=1.0, n_estimators=110;, score=1.000 total time=   0.2s\n","[CV 1/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 1/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 2/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 2/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 3/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 3/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 4/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 4/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 5/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 5/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=0.933 total time=   0.2s\n","[CV 6/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 6/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=0.800 total time=   0.2s\n","[CV 7/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 7/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=0.867 total time=   0.2s\n","[CV 8/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 8/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 9/10; 15/15] START learning_rate=1.0, n_estimators=120......................\n","[CV 9/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=1.000 total time=   0.2s\n","[CV 10/10; 15/15] START learning_rate=1.0, n_estimators=120.....................\n","[CV 10/10; 15/15] END learning_rate=1.0, n_estimators=120;, score=1.000 total time=   0.2s\n","{'learning_rate': 1.0, 'n_estimators': 90}\n","0.9533333333333334\n"]},{"output_type":"execute_result","data":{"text/plain":["AdaBoostClassifier(n_estimators=90)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(n_estimators=90)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(n_estimators=90)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"Phhlyr-H0yqB"},"source":["# II. Boosting 비교\n","\n","###Gradient boosting\n","- 앙상블 기법\n","- DT를 Boost하여 오차의 Gradient를 최소화\n","- Tree 모형을 기반으로 하는 앙상블\n","- 미분 가능한 loss 함수와 GD 최적화 알고리즘을 사용(=ANN)\n","- GBM의 하이퍼파라미터\n"," - The number of trees or estimators in the model.\n"," - The learning rate of the model.\n"," - The row and column sampling rate for stochastic models.\n"," - The maximum tree depth.\n"," - The minimum tree weight.\n"," - The regularization terms alpha and lambda."]},{"cell_type":"markdown","metadata":{"id":"_ZhD5CP63uSk"},"source":[" - loss : 경사 하강법에서 사용할 비용 함수. 기본값 'devidence'\n","\n"," - learning_rate : GBM이 학습을 진행할 때마다 적용하는 학습률. 기본값은  0.1\n","  - weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수\n","  - 0 ~ 1 사이의 값\n","  - 작을 경우, 예측 성능이 높아질 수 있으나 많은 week learner가 순차적인 반복이 필요해 수행시간이 길어지고 local optima에 빠질 수 있음\n","  - 너무 큰 값을 설정하는 경우, 최적값을 못찰을 수 있어 성능이 떨어질 가능성이 있으나, 시간은 단축\n","  - learning_rate은 n_estimator와 상호 보완적으로 활용\n","\n"," - n_estimators: weak learner의 개수이며,기본값은 100\n","  - weak learner가 많으면 성능 개선이 있지만 많은 시간 소요\n","\n","- subsample: weak learner가 학습에 사용하는 데이터 샘플링의 비율, 기본값은 1이며, 전체 데이터를 기반으로 학습함. 1보다 작은 값을 설정해서 과적합을 완화시킬 수 있음"]},{"cell_type":"markdown","metadata":{"id":"SL6teLzxg_Sv"},"source":["### 0. Adaboosting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlqgPJNQIIIG"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vtY0NeH5J6cH","outputId":"d08cedcc-f31c-497f-ad8a-4953b982de54"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.6625\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7625"]},"metadata":{},"execution_count":6}],"source":["# binary.csv 파일, 파티셔닝 8:2\n","binary = pd.read_csv(\"binary.csv\")\n","y = binary.admit\n","X = binary.drop(\"admit\", axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, stratify = y)\n","boost = AdaBoostClassifier().fit( X_train, y_train)\n","rf = RandomForestClassifier().fit( X_train, y_train)\n","\n","print( boost.score( X_test, y_test) )\n","rf.score( X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"8aKetD3Z0z1O"},"source":["### 1. Gradient Boosting\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"of6Ui7VV0z7t"},"outputs":[],"source":["from numpy import mean\n","from numpy import std\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from matplotlib import pyplot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ib56CRrr2Rwv"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2 )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYPekUad3m1A","outputId":"a54a2a2f-aa56-40ed-f70b-bfb8cd3a0f81"},"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 96, number of negative: 224\n","[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000072 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 105\n","[LightGBM] [Info] Number of data points in the train set: 320, number of used features: 3\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n","[LightGBM] [Info] Start training from score -0.847298\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"]}],"source":["#피팅\n","from sklearn.ensemble import AdaBoostClassifier\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","model = GradientBoostingClassifier(learning_rate=0.05,\n","                                   n_estimators=200).fit(X_train, y_train)\n","model2 = AdaBoostClassifier(learning_rate=0.05,\n","                                   n_estimators=200).fit( X_train, y_train)\n","model3 = RandomForestClassifier().fit( X_train, y_train)\n","model4 = XGBClassifier().fit( X_train, y_train)\n","model5 = LGBMClassifier().fit( X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Rx3TM9E4Yn9","outputId":"aa1f096a-acee-4e81-9390-26c6cf18fbfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.6125\n","0.625\n","0.6625\n","0.5375\n","0.6\n"]}],"source":["print(model.score(X_test, y_test))\n","print(model2.score(X_test, y_test))\n","print(model3.score(X_test, y_test))\n","print(model4.score(X_test, y_test))\n","print(model5.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7aG1JLR1rWs","outputId":"04469293-5f11-4283-b417-5f745cec84bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.696 (0.075)\n"]}],"source":["# k-foldCV\n","model = GradientBoostingClassifier()\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n","print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3ArSclH6U8M"},"outputs":[],"source":["#GridSearchCV\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","xparam = {\"learning_rate\":[0.05, 0.1, 0.15], \"n_estimators\":[50,100,150]}\n","xgb_cv = GridSearchCV( XGBClassifier(), xparam ).fit( X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p58TzVZS9I9o","outputId":"b742d383-1f39-422f-edd2-511634fb6c39"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6"]},"metadata":{},"execution_count":13}],"source":["best = xgb_cv.best_estimator_ #cv결과 중 제일 좋은 모형\n","best.score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"PhmUchYT1aAo"},"source":["### 3.Gradient Boosting With XGBoost\n","- Extreme Gradient Boosting을 의미하며, sklearn 대신 xgboost 라이브러리 이용\n","- 계산 효율성을 높이며, 성능도 개선\n","- colab에 설치되어 있으며 필요 시 !pip install xgboost\n","- XGBClassifier와 XGBregressor 제공\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kyA3Vi61aGx","outputId":"e9fe794e-c1d1-4320-e14d-82e20ea6af1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.933 (0.022)\n","Prediction: 1\n"]}],"source":["# 분류, xgboost for classification\n","from numpy import asarray\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_classification\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from matplotlib import pyplot\n","\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n","\n","# evaluate the model\n","model = XGBClassifier()\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n","print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","\n","# fit\n","model = XGBClassifier()\n","model.fit(X, y)\n","\n","# prediction\n","row = [2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]\n","row = asarray(row).reshape((1, len(row)))\n","yhat = model.predict(row)\n","print('Prediction: %d' % yhat)"]},{"cell_type":"markdown","metadata":{"id":"e4Fl3zOL1aMB"},"source":["### 4.Gradient Boosting With LightGBM\n","- Light Gradient Boosted Machine\n","- MS에 의해 개발, 효율성이 강조된 GBM\n","- 설치되어 있으며 필요 시 !pip install lightgbm\n","- LGBMClassifier , LGBMRegressor classes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jy2y5_ry1aR0","outputId":"22419a0b-b18e-46aa-922d-ed1297cef4c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.934 (0.021)\n","[LightGBM] [Info] Number of positive: 501, number of negative: 499\n","[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000151 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 1000, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501000 -> initscore=0.004000\n","[LightGBM] [Info] Start training from score 0.004000\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","Prediction: 1\n"]}],"source":["# lightgbm for classification\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_classification\n","from lightgbm import LGBMClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from matplotlib import pyplot\n","\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n","\n","# evaluate the model\n","model = LGBMClassifier()\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n","print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","\n","# fit\n","model = LGBMClassifier()\n","model.fit(X, y)\n","\n","# prediction\n","row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n","yhat = model.predict(row)\n","print('Prediction: %d' % yhat)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dH-62off1aWk"},"source":["### 5.Gradient Boosting with CatBoost\n","- CatBoost는 Yandex가 개발한 라이브러리\n","- 효율적으로 계산하여 속도 개선\n","- 범주형 X변수에 대해 잘 작동: Category Gradient Boosting\n","- 설치 안되어 있으므로, 다음을 이용해서 설치\n","- !pip install catboost\n","- CatBoostClassifier , CatBoostRegressor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8ZH8BXV23TZ","outputId":"63b56507-1dc1-43b1-ff31-cd10ee470149"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.3)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.43.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n"]}],"source":["!pip install catboost"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SYKkp3PW1ac5","outputId":"29c271d1-d119-4cb6-801b-53f60f39aba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.925 (0.025)\n","Prediction: 1\n"]}],"source":["# catboost for classification\n","from numpy import mean\n","from numpy import std\n","from sklearn.datasets import make_classification\n","from catboost import CatBoostClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import RepeatedStratifiedKFold\n","from matplotlib import pyplot\n","\n","# define dataset\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n","\n","# evaluate the model\n","model = CatBoostClassifier(verbose=0, n_estimators=100)\n","cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n","n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n","print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n","\n","# fit\n","model = CatBoostClassifier(verbose=0, n_estimators=100)\n","model.fit(X, y)\n","\n","# prediction\n","row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n","yhat = model.predict(row)\n","print('Prediction: %d' % yhat)\n"]},{"cell_type":"markdown","metadata":{"id":"1BkQudlm2h6e"},"source":["Papers\n","-\tStochastic Gradient Boosting, 2002.\n","-\tXGBoost: A Scalable Tree Boosting System, 2016.\n","-\tLightGBM: A Highly Efficient Gradient Boosting Decision Tree, 2017.\n","-\tCatBoost: gradient boosting with categorical features support, 2017.\n"]},{"cell_type":"markdown","metadata":{"id":"1ale48uD-ifh"},"source":["# III. 연습\n","1. sms.csv를 읽으셔서 type을 target으로 하는 분류 모형\n","- 파티션 8:2 / 모형: XGBoost, RandomForest\n","- 상대적으로 좋은 성능의 모형을 gridsearch\n","- 테스트셋 예측->classification_report출력"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NeX0Dlt44uCi"},"outputs":[],"source":["#1. sms.csv\n","sms = pd.read_csv(\"sms.csv\")\n","sms.dropna(inplace=True)\n","X = sms.drop(\"type\", axis=1)\n","y = sms.type\n","X_train,X_test,y_train, y_test = train_test_split( X,y,\n","                                                  test_size=0.2, stratify = y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KIeNNrnhF88q"},"outputs":[],"source":["model1 = XGBClassifier().fit(X_train, y_train)\n","model2 = RandomForestClassifier().fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0leNKqY0Geus","outputId":"7eef6466-c905-49f9-e8c1-a0a00affe86a"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.9484189280107648\n","0.9786947746131419\n"]}],"source":["print(model1.score(X_train,y_train))\n","print(model2.score(X_train,y_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uzSG-b_1G7Bj"},"outputs":[],"source":["cv1 = GridSearchCV( RandomForestClassifier(), {\"n_estimators\":[250,300,350],\n","                                               \"max_depth\":[2,3,4]}).fit(X_train, y_train)\n","best1 = cv1.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IK88sClHhkF","outputId":"d53cacee-752a-4f17-e2e5-1130c75c871b"},"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.88      1.00      0.93       966\n","           1       1.00      0.07      0.14       149\n","\n","    accuracy                           0.88      1115\n","   macro avg       0.94      0.54      0.54      1115\n","weighted avg       0.89      0.88      0.83      1115\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","print( classification_report(y_test, best1.predict(X_test)) )"]},{"cell_type":"markdown","metadata":{"id":"Ivz-wmggk4Ww"},"source":["2. train.csv를 읽으셔서 label을 target으로 하는 분류 모형\n","- 모형: XGBoost, RandomForest\n","- XGBoost, RandomForest 모형들을 gridsearch\n","- test.csv를 읽은 테스트셋에 대해서 예측->classification_report출력으로 두 모형을 비교"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I46Evx8dIk_y"},"outputs":[],"source":["#2. train.csv\n","train = pd.read_csv(\"train.csv\")\n","train.dropna(inplace=True)\n","X_train = train.drop(\"label\", axis=1)\n","y_train = train.label\n","\n","test = pd.read_csv(\"test.csv\")\n","X_test = test.drop(\"label\", axis=1)\n","y_test = test.label"]},{"cell_type":"code","source":["X_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dpI8mv6UqraP","outputId":"c8ddeabd-99fa-4b1f-cbc9-bb9d183bea30"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5000, 784)"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9X6LeM5Kmx1","colab":{"base_uri":"https://localhost:8080/","height":437},"outputId":"d552d8cf-eef8-495f-826d-612256846033"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 480x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","\n","plt.matshow(X_train.values[0].reshape(28,28), cmap=plt.cm.gray)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yOob4qX7JHVH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"08480288-ee89-462d-d332-57e118762f1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 2 folds for each of 1 candidates, totalling 2 fits\n","[CV 1/2] END learning_rate=0.1, n_estimators=100;, score=0.910 total time= 1.3min\n","[CV 2/2] END learning_rate=0.1, n_estimators=100;, score=0.911 total time= 1.1min\n","              precision    recall  f1-score   support\n","\n","           0       0.98      0.96      0.97       100\n","           1       0.96      0.98      0.97       100\n","           2       0.92      0.96      0.94       100\n","           3       0.96      0.90      0.93       100\n","           4       0.96      0.94      0.95       100\n","           5       0.93      0.96      0.95       100\n","           6       0.95      0.95      0.95       100\n","           7       0.95      0.95      0.95       100\n","           8       0.96      0.93      0.94       100\n","           9       0.90      0.94      0.92       100\n","\n","    accuracy                           0.95      1000\n","   macro avg       0.95      0.95      0.95      1000\n","weighted avg       0.95      0.95      0.95      1000\n","\n","Fitting 2 folds for each of 1 candidates, totalling 2 fits\n","[CV 1/2] END .....max_depth=3, n_estimators=200;, score=0.785 total time=   1.1s\n","[CV 2/2] END .....max_depth=3, n_estimators=200;, score=0.768 total time=   1.1s\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.95      0.88       100\n","           1       0.62      0.99      0.76       100\n","           2       0.90      0.76      0.83       100\n","           3       0.69      0.68      0.68       100\n","           4       0.81      0.78      0.80       100\n","           5       0.89      0.34      0.49       100\n","           6       0.79      0.87      0.83       100\n","           7       0.82      0.87      0.84       100\n","           8       0.89      0.73      0.80       100\n","           9       0.71      0.77      0.74       100\n","\n","    accuracy                           0.77      1000\n","   macro avg       0.79      0.77      0.77      1000\n","weighted avg       0.79      0.77      0.77      1000\n","\n"]}],"source":["cv3 = GridSearchCV(XGBClassifier(), {\"n_estimators\":[100], \"learning_rate\":[0.1]},\n","                   verbose=4, cv=2).fit(X_train, y_train)\n","best3 = cv3.best_estimator_\n","print( classification_report(y_test, best3.predict(X_test)) )\n","\n","cv4 = GridSearchCV(RandomForestClassifier(), {\"n_estimators\":[200], \"max_depth\":[3]},\n","                   verbose=4,cv=2).fit(X_train, y_train)\n","best4 = cv4.best_estimator_\n","print( classification_report(y_test, best4.predict(X_test)) )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKJh7WXK1pMs","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b7e2dd1-ba12-4f1e-f48a-65921b76f891"},"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction: 1\n"]}],"source":["# 10개값을 갖는 한 개의 X 샘플에 대한 범주 예측\n","row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n","yhat = model.predict(row)\n","print('Prediction: %d' % yhat)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}